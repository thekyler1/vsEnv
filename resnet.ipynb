{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotation_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotation_file, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slicer = slice(-9)\n",
    "        img_path1 = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0][slicer], self.img_labels.iloc[idx, 0])\n",
    "        img_path2 = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1][slicer], self.img_labels.iloc[idx, 1])\n",
    "        image1 = read_image(img_path1)\n",
    "        image2 = read_image(img_path2)\n",
    "        label = self.img_labels.iloc[idx, 2]\n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)\n",
    "            image2 = self.transform(image2)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(image1)\n",
    "        return image1, image2, label\n",
    "\n",
    "path_trainSet = f\"/Users/necatiisik/lfw_dataset/pairsDevTrain.txt\"\n",
    "path_testSet = f\"/Users/necatiisik/lfw_dataset/pairsDevTest.txt\"\n",
    "\n",
    "datasetPath = f\"/Users/necatiisik/lfw_dataset/lfw/\"\n",
    "trainLabelPath = f\"/Users/necatiisik/lfw_dataset/pair_train_data.csv\"\n",
    "testLabelPath = f\"/Users/necatiisik/lfw_dataset/pair_test_data.csv\"\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "  transforms.ToPILImage(),\n",
    "  transforms.Resize(size=140),  # Conver 140x140 input images\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      mean=[0.6071, 0.4609, 0.3944],  # Normalization settings for the model, the calculated mean and std values\n",
    "      std=[0.2457, 0.2175, 0.2129]     # for the RGB channels of the tightly-cropped glint360k face dataset\n",
    "  )\n",
    "])\n",
    "\n",
    "train_data = CustomImageDataset(trainLabelPath, datasetPath, transform=preprocess, target_transform=None)\n",
    "test_data = CustomImageDataset(testLabelPath, datasetPath, transform=preprocess, target_transform=None)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(dataset = train_data,\n",
    "                          batch_size = batch_size, \n",
    "                          shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_data,\n",
    "                         batch_size = batch_size, \n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, random, copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "        Siamese network for image similarity estimation.\n",
    "        The network is composed of two identical networks, one for each input.\n",
    "        The output of each network is concatenated and passed to a linear layer. \n",
    "        The output of the linear layer passed through a sigmoid function.\n",
    "        `\"FaceNet\" <https://arxiv.org/pdf/1503.03832.pdf>`_ is a variant of the Siamese network.\n",
    "        This implementation varies from FaceNet as we use the `ResNet-18` model from\n",
    "        `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_ as our feature extractor.\n",
    "        In addition, we aren't using `TripletLoss` as the MNIST dataset is simple, so `BCELoss` can do the trick.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # get resnet model\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "        # over-write the first conv layer to be able to read MNIST images\n",
    "        # as resnet18 reads (3,x,x) where 3 is RGB channels\n",
    "        # whereas MNIST has (1,x,x) where 1 is a gray-scale channel\n",
    "        # overruled.. get (3,x,x)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.fc_in_features = self.resnet.fc.in_features\n",
    "        \n",
    "        # remove the last layer of resnet18 (linear layer which is before avgpool layer)\n",
    "        self.resnet = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "\n",
    "        # add linear layers to compare between the features of the two images\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.fc_in_features * 2, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # initialize the weights\n",
    "        self.resnet.apply(self.init_weights)\n",
    "        self.fc.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.resnet(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # get two images' features\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "\n",
    "        # concatenate both images' features\n",
    "        output = torch.cat((output1, output2), 1)\n",
    "\n",
    "        # pass the concatenation to the linear layers\n",
    "        output = self.fc(output)\n",
    "\n",
    "        # pass the out of the linear layers to sigmoid layer\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    # we aren't using `TripletLoss` as the MNIST dataset is simple, so `BCELoss` can do the trick.\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for batch_idx, (images_1, images_2, targets) in enumerate(train_loader):\n",
    "        images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device, torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images_1, images_2).squeeze()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 2 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(images_1), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            # if args.dry_run:\n",
    "            #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # we aren't using `TripletLoss` as the MNIST dataset is simple, so `BCELoss` can do the trick.\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images_1, images_2, targets) in test_loader:\n",
    "            images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device, torch.float32)\n",
    "            outputs = model(images_1, images_2).squeeze()\n",
    "            test_loss += criterion(outputs, targets).sum().item()  # sum up batch loss\n",
    "            pred = torch.where(outputs > 0.5, 1, 0)  # get the index of the max log-probability\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # for the 1st epoch, the average loss is 0.0001 and the accuracy 97-98%\n",
    "    # using default settings. After completing the 10th epoch, the average\n",
    "    # loss is 0.0000 and the accuracy 99.5-100% using default settings.\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/yg/vjnq2s395_5652ng6cshx95h0000gn/T/ipykernel_3749/3421993037.py:57: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m scheduler \u001b[39m=\u001b[39m StepLR(optimizer, step_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(tot_epoch):\n\u001b[0;32m----> 7\u001b[0m     train(model, device, train_loader, optimizer, epoch)\n\u001b[1;32m      8\u001b[0m     test(model, device, test_loader)\n\u001b[1;32m      9\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m outputs \u001b[39m=\u001b[39m model(images_1, images_2)\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m---> 11\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     12\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/functional.py:3095\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3092\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3093\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3095\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "\n",
    "tot_epoch  = 10\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "for epoch in range(tot_epoch):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"siamese_network.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
