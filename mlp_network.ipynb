{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotation_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotation_file, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slicer = slice(-9)\n",
    "        img_path1 = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0][slicer], self.img_labels.iloc[idx, 0])\n",
    "        img_path2 = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1][slicer], self.img_labels.iloc[idx, 1])\n",
    "        image1 = read_image(img_path1)\n",
    "        image2 = read_image(img_path2)\n",
    "        label = self.img_labels.iloc[idx, 2]\n",
    "        \n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)\n",
    "            image2 = self.transform(image2)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(image1)\n",
    "        #return image1, image2, label, img_path1, img_path2        \n",
    "        return image1, image2, label\n",
    "\n",
    "path_trainSet = f\"/Users/necatiisik/lfw_dataset/pairsDevTrain.txt\"\n",
    "path_testSet = f\"/Users/necatiisik/lfw_dataset/pairsDevTest.txt\"\n",
    "\n",
    "datasetPath = f\"/Users/necatiisik/lfw_dataset/lfw/\"\n",
    "trainLabelPath = f\"/Users/necatiisik/lfw_dataset/pair_train_data.csv\"\n",
    "testLabelPath = f\"/Users/necatiisik/lfw_dataset/pair_test_data.csv\"\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "  transforms.ToPILImage(),\n",
    "  transforms.Resize(size=140),  # Conver 140x140 input images\n",
    "  transforms.ToTensor()\n",
    "#   transforms.Normalize(\n",
    "#       mean=[0.6071, 0.4609, 0.3944],  # Normalization settings for the model, the calculated mean and std values\n",
    "#       std=[0.2457, 0.2175, 0.2129]     # for the RGB channels of the tightly-cropped glint360k face dataset\n",
    "#   )\n",
    "])\n",
    "\n",
    "train_data = CustomImageDataset(trainLabelPath, datasetPath, transform=preprocess, target_transform=None)\n",
    "test_data = CustomImageDataset(testLabelPath, datasetPath, transform=preprocess, target_transform=None)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(dataset = train_data,\n",
    "                          batch_size = batch_size, \n",
    "                          shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_data,\n",
    "                         batch_size = batch_size, \n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse, random, copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class MLPnetwork(nn.Module):\n",
    "    \"\"\"\n",
    "        Siamese network for image similarity estimation.\n",
    "        The network is composed of two identical networks, one for each input.\n",
    "        The output of each network is concatenated and passed to a linear layer. \n",
    "        The output of the linear layer passed through a sigmoid function.\n",
    "        `\"FaceNet\" <https://arxiv.org/pdf/1503.03832.pdf>`_ is a variant of the Siamese network.\n",
    "        This implementation varies from FaceNet as we use the `ResNet-18` model from\n",
    "        `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_ as our feature extractor.\n",
    "        In addition, we aren't using `TripletLoss` as the MNIST dataset is simple, so `BCELoss` can do the trick.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MLPnetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(140 * 140 * 3, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 1)\n",
    "    )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # initialize the weights\n",
    "        self.layers.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.layers(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # get two images' features\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "\n",
    "        # concatenate both images' features\n",
    "        output = torch.cat((output1, output2), 1)\n",
    "\n",
    "        # pass the concatenation to the linear layers\n",
    "        output = self.fc(output)\n",
    "\n",
    "        # pass the out of the linear layers to sigmoid layer\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 30,237,697\n"
     ]
    }
   ],
   "source": [
    "model = MLPnetwork().to(device)\n",
    "\n",
    "def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_param = count_parameters(model)\n",
    "\n",
    "print(f\"Number of parameters: {num_param:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
